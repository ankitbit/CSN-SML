{
    "collab_server" : "",
    "contents" : "library(stats)\nlibrary(lmtest)\nlibrary(caret)\nlibrary(car)\n\nfind_starts_1 = function(dat){\n  linear =  lm(log(mean_length) ~ log(vertices), dat)\n  inter = linear$coefficients[1]/log(2)\n  return(inter)\n}\n\nfind_starts_2 = function(dat){\n  linear =  lm(log(mean_length) ~ log(vertices), dat)\n  slope = linear$coefficients[2]\n  inter =  exp(linear$coefficients[1])\n  return(c(slope, inter))\n}\n\nfind_starts_3 = function(dat){\n  linear = lm(log(mean_length) ~ vertices, dat)\n  inter = exp(linear$coefficients[1])\n  slope = linear$coefficients[2]\n  return(c(slope, inter))\n}\n\n\ngroup_variance = function(table, lang){\n  var_g = c()\n  vert = sort(unique(table$vertices))\n  for (v in 1:length(vert)){\n    var_g[v] = var(table$mean_length[table$vertices == vert[v]])\n  }\n  data = data.frame(cbind(vert, var_g))\n  data[is.na(data)] = 0.0\n  # data[is.na(data)] = mean(data$var_g, na.rm = TRUE)\n  # data = data[complete.cases(data), ]\n  test = cor.test(data$vert, log(data$var_g), method = \"kendall\")\n  plot(vert, log(var_g), main = lang)\n  if (test$p.value > 0.05) return(TRUE)\n  else return(FALSE)\n}\n\n\n# Compute the residual standard error\ns_func = function(nlm){\n  sqrt(deviance(nlm)/df.residual(nlm))\n}\n\nsource = read.table(\"list.txt\", \n                    header = TRUE,               \n                    as.is = c(\"language\",\"file\")\n)\n\nlangs = source$language\n\n# Datasets with all the results\nresults = data.frame()\ndeviance = data.frame()\nAIC = data.frame()\nS = data.frame()\n\n# Computing all the results for each language\nfor (lang in 1:length(langs)) {\n  i = 1\n  j = 1\n  \n  # lists of results for each language\n  res = c()\n  dev = c()\n  aic = c()\n  s_list = c()\n  \n  lang_name = source$language[lang]\n  lang_table = read.table(source$file[lang], header = FALSE)\n  colnames(lang_table) = c(\"vertices\",\"degree_2nd_moment\", \"mean_length\")\n  \n  lmod1 = lm(mean_length ~ vertices, lang_table)\n  \n  if (group_variance(lang_table, lang) == FALSE){\n    lang_table = aggregate(lang_table, list(lang_table$vertices), mean)\n    paste(cat(\"The\", langs[lang], \"language has not the homoskedasticity\\n\"))\n  }\n  else {\n    paste(cat(\"The\", langs[lang], \"language HAS the homoskedasticity property\\n\"))\n  }\n  \n  ####### model 0 #######\n  # formula0 = (n+1)/3\n  \n  RSS <- sum((lang_table$mean_length-(lang_table$vertices+1)/3)^2)\n  n <- length(lang_table$vertices)\n  p <- 0\n  s_0 <- sqrt(RSS/(n - p))\n  \n  aic_0 <- n*log(2*pi) + n*log(RSS/n) + n + 2*(p + 1)\n  aic[j] = aic_0\n  dev[j] = RSS\n  s_list[j] = s_0\n  j = j + 1\n  \n  \n  ######## model 1 #######\n  # formula1 = function(n, b) (n/2)^b \n  # b*log(n) - b*log(2)\n  \n  b1_init = find_starts_1(lang_table)\n  \n  mod1 = nls(mean_length ~ (vertices/2)^b, lang_table, \n             start = list(b = b1_init), trace = FALSE)\n  \n  b1_opt = coef(mod1)[\"b\"]\n  \n  res[i] = b1_opt\n  i = i + 1\n  \n  aic[j] = AIC(mod1)\n  dev[j] = deviance(mod1)\n  s_list[j] = s_func(mod1)\n  j = j + 1\n  \n  ######## model 2 #######\n  # b*log(n) + log(a)\n  \n  b2_init = find_starts_2(lang_table)[1]\n  a2_init = find_starts_2(lang_table)[2]\n  \n  mod2 = nls(mean_length ~ a*(vertices^b), lang_table,\n             start = list(a = a2_init, b = b2_init), trace = FALSE)\n  \n  a2_opt = coef(mod2)[\"a\"]\n  b2_opt = coef(mod2)[\"b\"]\n  \n  res[i] = a2_opt\n  i = i + 1\n  res[i] = b2_opt\n  i = i + 1\n  \n  dev[j] = deviance(mod2)\n  aic[j] = AIC(mod2)\n  s_list[j] = s_func(mod2)\n  j  = j +1\n  \n  \n  ####### model 3 ######\n  c3_init = find_starts_3(lang_table)[1]\n  a3_init = find_starts_3(lang_table)[2]\n  \n  mod3 = nls(mean_length ~ a * exp(c*vertices), lang_table,\n             start = list(a = a3_init, c= c3_init), trace = FALSE)\n  \n  a3_opt = coef(mod3)[\"a\"]\n  c3_opt = coef(mod3)[\"c\"]\n  \n  res[i] = a3_opt\n  i = i + 1\n  res[i] = c3_opt\n  i = i + 1\n  \n  dev[j] = deviance(mod3)\n  aic[j] = AIC(mod3)\n  s_list[j] = s_func(mod3)\n  j  = j +1\n  \n  \n  ###### model 1+ #######\n  # f(n) = (n/2)^b + d\n  \n  b1p_init = b1_opt\n  d1p_init = 0\n  \n  mod1p = nls(mean_length ~ (vertices/2)^b + d, lang_table,\n              start = list(b = b1p_init, d = d1p_init), trace = FALSE)\n  \n  b1p_opt = coef(mod1p)[\"b\"]\n  d1p_opt = coef(mod1p)[\"d\"]\n  \n  \n  res[i] = b1p_opt\n  i = i + 1\n  res[i] = d1p_opt\n  i = i + 1\n  \n  dev[j] = deviance(mod1p)\n  aic[j] = AIC(mod1p)\n  s_list[j] = s_func(mod1p)\n  j  = j +1\n  \n  \n  ###### model 2+ #######\n  # f(n) = a*n^b + d\n  \n  a2p_init = a2_opt\n  b2p_init = b2_opt\n  d2p_init = 1.5   # seems that 1.5 work well with almost all the languages (Chinese critic language)\n  \n  mod2p = nls(mean_length ~ a * vertices^b + d, lang_table,\n              start = list(a = a2p_init, b = b2p_init, d = d2p_init), trace = FALSE)\n  \n  a2p_opt = coef(mod2p)[\"a\"]\n  b2p_opt = coef(mod2p)[\"b\"]\n  d2p_opt = coef(mod2p)[\"d\"]\n  \n  res[i] = a2p_opt\n  i = i + 1\n  res[i] = b2p_opt\n  i = i + 1\n  res[i] = d2p_opt\n  i = i + 1\n  \n  dev[j] = deviance(mod2p)\n  aic[j] = AIC(mod2p)\n  s_list[j] = s_func(mod2p)\n  j  = j + 1\n  \n  \n  ##### populate dataframes #####\n  \n  results = rbind(results, res)\n  deviance = rbind(deviance, dev)\n  AIC = rbind(AIC, aic)\n  S = rbind(S, s_list)\n  \n  models = c(\"mod0\", \"mod1\", \"mod2\", \"mod3\", \"mod1p\", \"mod2p\")\n  min_aic = which.min(aic)\n  min_s = which.min(s_list)\n  if (min_aic == min_s) {\n    print(models[min_aic])\n    barplotfile = sprintf(\"plot_%s.pdf\",lang_name)\n    pdf(file = barplotfile)    # open pdf file\n    plottitle = sprintf(\"Best fit for %s\",lang_name)\n    plot(log(lang_table$vertices), log(lang_table$mean_length),\n         xlab = \"log(vertices)\", ylab = \"log(mean dependency length)\", main = plottitle)\n    lines(log(sort(lang_table$vertices)), log(fitted(get(models[min_aic]))[order(lang_table$vertices)]), col = \"green\")\n    # plot(lang_table$vertices, lang_table$mean_length,\n    #      xlab = \"log(vertices)\", ylab = \"log(mean dependency length)\", main = plottitle)\n    # lines(sort(lang_table$vertices), fitted(get(models[min_aic]))[order(lang_table$vertices)], col = \"green\")\n    grid()\n    dev.off()\n  }\n  else {\n    print(c(min_aic, min_s))\n    n = length(aic)\n    sec_min_aic = sort(aic, decreasing = TRUE)[n-1]\n    if (min_s == which(aic == sec_min_aic)){\n      print(models[which(aic == sec_min_aic)])\n      print(models[min_s])\n      barplotfile = sprintf(\"plot_%s.pdf\",lang_name)\n      pdf(file = barplotfile)    # open pdf file\n      plottitle = sprintf(\"Best fit for %s\",lang_name)\n      plot(log(lang_table$vertices), log(lang_table$mean_length),\n           xlab = \"log(vertices)\", ylab = \"log(mean dependency length)\", main = plottitle)\n      lines(log(lang_table$vertices), log(fitted(get(models[min_s]))), col = \"blue\")\n      grid()\n      dev.off()\n    }\n  }\n}\n\n\ncolnames(results) = c(\"1.b\", \"2.a\", \"2.b\", \"3.a\", \"3.c\", \"1+.b\", \"1+.d\", \"2+.a\", \"2+.b\", \"2+.d\")\ncolnames(deviance) = c(\"0\",\"1\",\"2\", \"3\", \"1+\",\"2+\")\ncolnames(AIC) = c(\"0\",\"1\",\"2\", \"3\", \"1+\",\"2+\")\ncolnames(S) = c(\"0\",\"1\",\"2\", \"3\", \"1+\",\"2+\")\nrownames(results) = langs\nrownames(deviance) = langs\nrownames(AIC) = langs\nrownames(S) = langs\n\n\n\n\n###### Table wit DELTA (difference between AIC and min(AIC) for each language) ######\ndeltaAIC = function(aic_tab){\n  del_aic = data.frame(row.names = langs)\n  for (row in 1:nrow(aic_tab)){\n    diff_vec = aic_tab[row,] - min(aic_tab[row,])\n    del_aic = rbind(del_aic, diff_vec)\n  }\n  colnames(del_aic) = c(\"0\",\"1\",\"2\", \"3\", \"1+\",\"2+\")\n  return(del_aic)\n}\n\ndelta_AIC = deltaAIC(AIC)\n\n\n\n\n# Prova sul primo dataset  ------------------------------------------------\n\ngroup_variance1 = function(table, lang){\n  var_g = c()\n  vert = sort(unique(table$vertices))\n  for (v in 1:length(vert)){\n    var_g[v] = var(table$mean_length[table$vertices == vert[v]])\n  }\n  data = data.frame(cbind(vert, var_g))\n  # data = data[complete.cases(data), ]\n  # data[is.na(data)] = mean(data$var_g, na.rm = TRUE)\n  return(data)\n  # var_g[is.na(var_g)]\n  # test = cor.test(data$vert, data$var_g, method = \"spearman\")\n  # plot(vert, var_g, main = lang)\n  # print(test$p.value)\n  # if (test$p.value > 0.05) return(TRUE)\n  # else return(FALSE)\n}\n\nn = 1\nprimo_table = read.table(source$file[n], header = FALSE)\ncolnames(primo_table) = c(\"vertices\",\"degree_2nd_moment\", \"mean_length\")\nd = group_variance1(primo_table,n)\n# d = d[complete.cases(d), ]\nd[is.na(d)] = 0# mean(d$var_g, na.rm = TRUE)\n\n\nif(!require(corrplot)){\n  install.packages(\"corrplot\")\n}else{\n  library(corrplot)\n}\n\nc = cor(d)\ncorrplot::corrplot(c)\n\nbarplotfile = sprintf(\"plot_%s.png\",lang_name)\npng(file = barplotfile)    # open pdf file\nplottitle = sprintf(\"Best fit for %s\",lang_name)\nplot(log(primo_table$vertices), log(primo_table$mean_length),\n     xlab = \"log(vertices)\", ylab = \"log(mean dependency length)\", main = plottitle)\nlines(log(primo_table$vertices), log(fitted(get(models[min_aic]))), col = \"green\")\ndev.off()\n",
    "created" : 1514997374112.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2734814015",
    "id" : "5ADB8FE0",
    "lastKnownWriteTime" : 1510824475,
    "last_content_update" : 1510824475,
    "path" : "~/CSN-4/lab4.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}